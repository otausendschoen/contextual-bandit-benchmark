{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6dcea14-e36c-4a63-8431-6c30c6264642",
   "metadata": {},
   "source": [
    "![bse_logo_textminingcourse](https://bse.eu/sites/default/files/bse_logo_small.png)\n",
    "\n",
    "# *Final Project: Contextual Bandits - Preprocessing Context Features*\n",
    "\n",
    "## Reinforcement Learning\n",
    "\n",
    "#### Authors: **Timothy Cassel, Marvin Ernst, Oliver Tausendschön**\n",
    "\n",
    "Date: July 2, 2025\n",
    "\n",
    "Instructors: *Hamish Flynn and Vincent Adam*\n",
    "\n",
    "In this notebook, we preprocess the context features from the **Open Bandit Dataset (OBD)** to improve the performance of contextual bandit algorithms. The original context vectors are **high-dimensional and sparse**, with many features rarely active. This motivates a **dimensionality reduction step** (via PCA) to extract denser and more informative representations.\n",
    "\n",
    "We also construct a **reduced-action dataset**, where only **10 arms** (the 5 most-clicked and 5 least-clicked) are retained. This speeds up experimentation while preserving diversity in action outcomes.\n",
    "\n",
    "Finally, we extract the **three most frequently selected arms** under the logging policy. These arms are useful for focused offline evaluation, as they appear more often in the dataset and increase the chance that our evaluation policy matches logged actions - making **IPS and DR estimates more reliable**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f66de-5735-40b9-969d-71ceeeccb077",
   "metadata": {},
   "source": [
    "## Step 1: Load the Dataset\n",
    "\n",
    "Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19025d28-eda8-44db-882d-5e51286c19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_opb import load_obp_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the OBP-style dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e03178a-b276-4692-899b-fa1598d3dcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded OBP dataset: 1374327 rounds, 80 actions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Admin/Documents/Git/BSE/RL/contextual-bandit-benchmark/.venv/lib/python3.12/site-packages/obp/dataset/real.py:192: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  item_feature_cat = self.item_context.drop(\"item_feature_0\", 1).apply(\n",
      "/Users/Admin/Documents/Git/BSE/RL/contextual-bandit-benchmark/.venv/lib/python3.12/site-packages/obp/dataset/real.py:195: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  self.action_context = pd.concat([item_feature_cat, item_feature_0], 1).values\n"
     ]
    }
   ],
   "source": [
    "IS_RAW = False\n",
    "\n",
    "data = load_obp_dataset(\n",
    "    campaign=\"all\",\n",
    "    behavior_policy=\"random\",\n",
    "    n_rounds=None,\n",
    "    use_raw_csv=IS_RAW\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9812c8-e731-4a58-a6f1-af403a556ffd",
   "metadata": {},
   "source": [
    "## Step 2: Drop Sparse Context Features\n",
    "\n",
    "We drop features that are active in fewer than `threshold` of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a4ec147-1c2b-4041-9292-ca360862e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_sparse_features(X, threshold=0.01):\n",
    "    feature_means = X.mean(axis=0)\n",
    "    keep_mask = feature_means >= threshold\n",
    "    print(f\"Keeping {keep_mask.sum()} out of {len(keep_mask)} features\")\n",
    "    return X[:, keep_mask], keep_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d44fd6f-41af-4d40-bc24-4ed64652f485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping 18 out of 26 features\n"
     ]
    }
   ],
   "source": [
    "context_matrix = data[\"context\"]\n",
    "context_dropped, keep_mask = drop_sparse_features(context_matrix, threshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92cdaac-84bc-4bf3-9efe-443a1995cb23",
   "metadata": {},
   "source": [
    "## Step 3: PCA for Dimensionality Reduction\n",
    "\n",
    "We apply PCA to retain `variance_threshold` of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97c0a342-9b66-4b3e-b743-cccbdcdd064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X, variance_threshold=0.99):\n",
    "    pca = PCA(n_components=variance_threshold)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    print(f\"Reduced dimensions from {X.shape[1]} to {X_reduced.shape[1]}\")\n",
    "    return X_reduced, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "471910f2-6bb1-4e13-bbd5-4250d2fdfe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dimensions from 18 to 12\n"
     ]
    }
   ],
   "source": [
    "X_reduced, pca_model = apply_pca(context_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd3695-e912-48fd-8d6e-ed63d4bdbc3f",
   "metadata": {},
   "source": [
    "## Step 4: Save Preprocessed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fba5c0c1-dcbd-4129-9416-9e8b16af1279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed data to 'processed/'\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(\"processed\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "np.save(output_dir / \"context_reduced.npy\", X_reduced)\n",
    "np.save(output_dir / \"context_mask.npy\", keep_mask)\n",
    "np.save(output_dir / \"actions.npy\", data[\"action\"])\n",
    "np.save(output_dir / \"rewards.npy\", data[\"reward\"])\n",
    "np.save(output_dir / \"pscores.npy\", data[\"pscore\"])\n",
    "\n",
    "with open(output_dir / \"meta.txt\", \"w\") as f:\n",
    "    f.write(f\"n_actions: {data['n_actions']}\\n\")\n",
    "\n",
    "print(\"Saved preprocessed data to 'processed/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Reduced Action Dataset (10 Arms Only)\n",
    "\n",
    "We select the 5 most-clicked and 5 least-clicked arms, and keep all observations for those actions only.\n",
    "\n",
    "**1: Compute click count per action**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_actions, click_rates = np.unique(data['action'], return_counts=True)\n",
    "rewards = data[\"reward\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute click-through rates per action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks_per_action = np.array([rewards[data['action'] == a].sum() for a in unique_actions])\n",
    "ctr_per_action = clicks_per_action / click_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select top 2 and bottom 2 arms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Arms (original IDs): [ 4 35 54 60]\n"
     ]
    }
   ],
   "source": [
    "sorted_indices = np.argsort(ctr_per_action)\n",
    "low_arms = unique_actions[sorted_indices[:5]]\n",
    "high_arms = unique_actions[sorted_indices[-5:]]\n",
    "\n",
    "selected_arms = np.sort(np.concatenate([low_arms, high_arms]))\n",
    "print(\"Selected Arms (original IDs):\", selected_arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2: Filter dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.isin(data[\"action\"], selected_arms)\n",
    "filtered_context = data[\"context\"][mask]\n",
    "filtered_action = data[\"action\"][mask]\n",
    "filtered_reward = data[\"reward\"][mask]\n",
    "filtered_pscore = data[\"pscore\"][mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reindex actions to 0–9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_to_reduced = {orig: idx for idx, orig in enumerate(selected_arms)}\n",
    "reduced_action = np.array([original_to_reduced[a] for a in filtered_action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3: Save reduced dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved reduced dataset to 'processed_small/'\n"
     ]
    }
   ],
   "source": [
    "output_dir_small = Path(\"processed_small\")\n",
    "os.makedirs(output_dir_small, exist_ok=True)\n",
    "\n",
    "np.save(\"processed_small/context_reduced.npy\", X_reduced[mask])\n",
    "np.save(\"processed_small/actions.npy\", reduced_action)\n",
    "np.save(\"processed_small/rewards.npy\", filtered_reward)\n",
    "np.save(\"processed_small/pscores.npy\", filtered_pscore)\n",
    "\n",
    "with open(\"processed_small/meta.txt\", \"w\") as f:\n",
    "    f.write(f\"original_actions: {selected_arms.tolist()}\\n\")\n",
    "    f.write(\"n_actions: 10\\n\")\n",
    "\n",
    "print(\"Saved reduced dataset to 'processed_small/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create Focused Dataset (Top 3 Most Frequently Logged Arms)\n",
    "\n",
    "We extract the **three most frequently selected arms** under the random logging policy. This subset increases the chance of action overlap and enables more reliable IPS/DR evaluation.\n",
    "\n",
    "Count frequency of each action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_actions, counts = np.unique(data[\"action\"], return_counts=True)\n",
    "sorted_indices = np.argsort(counts)[::-1]\n",
    "top3_arms = unique_actions[sorted_indices[:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter dataset for those arms only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_top3 = np.isin(data[\"action\"], top3_arms)\n",
    "context_top3 = data[\"context\"][mask_top3]\n",
    "action_top3 = data[\"action\"][mask_top3]\n",
    "reward_top3 = data[\"reward\"][mask_top3]\n",
    "pscore_top3 = data[\"pscore\"][mask_top3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reindex actions to 0–2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_to_top3 = {orig: idx for idx, orig in enumerate(top3_arms)}\n",
    "action_top3_reindexed = np.array([original_to_top3[a] for a in action_top3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to new directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved focused dataset with top 3 arms to 'processed_top3/'\n"
     ]
    }
   ],
   "source": [
    "output_dir_top3 = Path(\"processed_top3\")\n",
    "output_dir_top3.mkdir(exist_ok=True)\n",
    "\n",
    "np.save(output_dir_top3 / \"context_reduced.npy\", X_reduced[mask_top3])\n",
    "np.save(output_dir_top3 / \"actions.npy\", action_top3_reindexed)\n",
    "np.save(output_dir_top3 / \"rewards.npy\", reward_top3)\n",
    "np.save(output_dir_top3 / \"pscores.npy\", pscore_top3)\n",
    "\n",
    "with open(output_dir_top3 / \"meta.txt\", \"w\") as f:\n",
    "    f.write(f\"original_actions: {top3_arms.tolist()}\\n\")\n",
    "    f.write(\"n_actions: 3\\n\")\n",
    "\n",
    "print(\"Saved focused dataset with top 3 arms to 'processed_top3/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff005295-ae2a-4ed9-a7b9-5d9668a9af06",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Dropped rarely active context features (<1% activation)\n",
    "- Applied PCA to reduce dimensionality while retaining 99% variance\n",
    "- Created **reduced dataset** with 10 selected arms (5 most-clicked + 5 least-clicked)\n",
    "- Created **focused dataset** with the 3 most frequently selected arms under the random logging policy\n",
    "- Saved all cleaned datasets for fast evaluation and reliable offline policy assessment\n",
    "\n",
    "This preprocessing pipeline supports both:\n",
    "- **Full-data training and benchmarking**, and\n",
    "- **Faster, more statistically robust experimentation** using smaller action subsets that improve offline evaluation reliability (e.g., IPS, DR)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
